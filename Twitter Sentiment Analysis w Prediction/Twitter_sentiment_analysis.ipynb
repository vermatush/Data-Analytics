{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpnyUEJrAjE"
      },
      "source": [
        "\n",
        "# **Twitter Sentiment Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2zz0qU6Ut6g"
      },
      "source": [
        "##  Table of Contents:\n",
        "* [Import data and modules](#first-bullet)\n",
        "* [Data Cleaning](#second-bullet)\n",
        "* [Exploratory Analysis](#third-bullet)\n",
        "    * [Classify data into parties](#section_3_1)\n",
        "* [Sentiment Classification](#fourth-bullet)\n",
        "    * [Machine learning models using Tf-idf](#section_4_1)\n",
        "    * [Deep Learning models using Tf (Bag of words)](#section_4_2)\n",
        "    * [Comparing Models](#section_4_3)\n",
        "    * [Best Model Training](#section_4_4)\n",
        "    * [Fitting on elections data](#section_4_5)\n",
        "* [Negative Reason Analysis](#Fifth-bullet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOuRc3AUIfLJ"
      },
      "outputs": [],
      "source": [
        "#Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "\n",
        "import re\n",
        "import string\n",
        "import html\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from IPython.display import Markdown, display\n",
        "%matplotlib inline\n",
        "\n",
        "import io\n",
        "from IPython.display import HTML\n",
        "from pandas import DataFrame\n",
        "\n",
        "#Installing required libraries\n",
        "#!pip install <library_name>\n",
        "\n",
        "try:\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, auc\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    from sklearn.feature_selection import chi2, SelectFromModel, SelectKBest\n",
        "    from sklearn.feature_selection import RFE\n",
        "    from sklearn.model_selection import train_test_split,GridSearchCV, StratifiedKFold, KFold\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    \n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn import ensemble\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, label_binarize, MinMaxScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "except:\n",
        "    !pip install -U scikit-learn\n",
        "    \n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, auc\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    from sklearn.feature_selection import chi2, SelectFromModel, SelectKBest\n",
        "    from sklearn.feature_selection import RFE\n",
        "    from sklearn.model_selection import train_test_split,GridSearchCV, StratifiedKFold, KFold\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    \n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn import ensemble\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, label_binarize, MinMaxScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "try:\n",
        "  import imblearn\n",
        "  from imblearn.over_sampling import RandomOverSampler\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "except:\n",
        "  !pip install imbalanced-learn\n",
        "  import imblearn\n",
        "  from imblearn.over_sampling import RandomOverSampler\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  from imblearn.under_sampling import RandomUnderSampler\n",
        "  \n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from pywaffle import Waffle\n",
        "except:\n",
        "    !pip install pywaffle\n",
        "    from pywaffle import Waffle\n",
        "\n",
        "try:\n",
        "    from statsmodels.graphics.gofplots import qqplot\n",
        "    import statsmodels.api as sm\n",
        "except:\n",
        "    !pip install statsmodels\n",
        "    from statsmodels.graphics.gofplots import qqplot\n",
        "\n",
        "try:\n",
        "  from xgboost import XGBClassifier\n",
        "except:\n",
        "  !pip install xgboost\n",
        "  from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "try:\n",
        "  from venn import venn\n",
        "except:\n",
        "  !pip install venn\n",
        "  from venn import venn\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "import torch\n",
        "from scipy.sparse import coo_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IRz_NQOhzQe"
      },
      "outputs": [],
      "source": [
        "#NLTK Library\n",
        "try:\n",
        "  import nltk\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "except:\n",
        "  !pip install nltk\n",
        "  import nltk\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\") \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4aMu12KoS-n"
      },
      "source": [
        "# Section 1 : Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHgDv9nKWLo"
      },
      "source": [
        "## 1.1 Loading and exploring Data \n",
        "a) Load sentiment_analysis.csv and Canadian_elections_2021.csv "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KCryQjtGNTB"
      },
      "outputs": [],
      "source": [
        "# load 'sentiment_analysis.csv' to Google Colab or 'sentiment_analysis.csv' file in the session before running#\n",
        "try:\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuHu2pv0tvwC"
      },
      "outputs": [],
      "source": [
        "# load 'Canadian_elections_2021.csv' to Google Colab or 'Canadian_elections_2021.csv' file in the session before running#\n",
        "try:\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqT4jckDtpWZ"
      },
      "outputs": [],
      "source": [
        "#load 'sentiment_analysis.csv' and 'Canadian_elections_2021.csv' files in the session before running#\n",
        "#Load files and into dateframe\n",
        "\n",
        "senti_df = pd.read_csv(('sentiment_analysis.csv'), low_memory=False)\n",
        "election_df = pd.read_csv(('Canadian_elections_2021.csv'), low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y4i1bKjA8pQ"
      },
      "outputs": [],
      "source": [
        "# Copy of both dataset for further use\n",
        "senti_df_cp = senti_df.copy()\n",
        "election_df_cp= election_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQTT392SzlgH"
      },
      "outputs": [],
      "source": [
        "#Explore dataset\n",
        "print('sentiment data column',senti_df.columns)\n",
        "display('\\n',senti_df.info)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------\\n\")\n",
        "\n",
        "print('election data columns\\n',election_df.columns)\n",
        "display('\\n',election_df.info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK2SWKceskdk"
      },
      "outputs": [],
      "source": [
        "# Sample data\n",
        "print('sentiment data\\n') \n",
        "display(senti_df.head(4))\n",
        "\n",
        "print('\\nElection data')\n",
        "display(election_df.head(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qoiMg52v6eI"
      },
      "outputs": [],
      "source": [
        "#Plotting graph from the dataframe provided\n",
        "sns.countplot(x = 'label', data = senti_df)\n",
        "plt.title(\"Labels distribution\")\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv0iiME7wmLn"
      },
      "outputs": [],
      "source": [
        "#Top 15 tweeted hashtags in the elections dataset\n",
        "election_df.text.str.extractall(r'(\\#\\w+)')[0].value_counts()[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGs-KpGVwzMP"
      },
      "outputs": [],
      "source": [
        "#Top 15 mentions in the elections dataset\n",
        "election_df.text.str.extractall(r'(\\@\\w+)')[0].value_counts()[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c5b1e5"
      },
      "source": [
        "##1.2 Cleaning dataset \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcJrpf0VUt6n"
      },
      "source": [
        "Observing the sentiments dataset, they contain a combination of emoji's, hyperlinks and stop words. The most important step for this assignment is to clean the data before analysis of the tweets. The clean_data function handles the following:\n",
        "1. Converts emoji's to text\n",
        "2. Removes hyperlinks\n",
        "3. Convert all HTML text to ASCII\n",
        "4. Remove all numeric only data.\n",
        "5. Convert text to lowercase.\n",
        "6. Remove all stop-words\n",
        "7. Remove all punctuations.\n",
        "8. Removes twitter handles\n",
        "9. Remove all tags\n",
        "\n",
        "The assignment required to assign the tweets to a particular party in the later tasks, '#' and '@' were excluded while removing the punctuations in first excecution. These are removed later once the tweets were assigned to particular parties before the model implementation.\n",
        "\n",
        "Also additional stop word 'rt' is added to list of Standard stopwords imported from NLTK library "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_6dW45FrfGZ"
      },
      "outputs": [],
      "source": [
        "#Using the stopwords\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "#Add other required stopwords  \n",
        "stops.append('rt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-mDWSZ25bQO"
      },
      "outputs": [],
      "source": [
        "def cleaned_data(dataset, rmove_handle_tag = False):\n",
        "\n",
        "  #remove html tags and attributes\n",
        "  dataset = re.sub('(<[^>]*>)','',dataset)\n",
        "\n",
        "  #Convert_emoji's to text\n",
        "  dataset = emoji.demojize(dataset)\n",
        "\n",
        "  #Convert character codes to string\n",
        "  dataset = html.unescape(dataset)\n",
        "\n",
        "  #remove urls\n",
        "  dataset = re.sub(\"(\\w+:\\/\\/\\S+)|(http[^\\s]+)|(www.[^\\s]+)\",'',dataset, flags=re.MULTILINE)\n",
        "\n",
        "  #lower case\n",
        "  dataset = dataset.lower()\n",
        "\n",
        "  if rmove_handle_tag==False:\n",
        "    #remove punctuations apart from the @ and # which can be useful later\n",
        "    dataset = dataset.translate(str.maketrans('', '', re.sub('#|@','',(string.punctuation))))\n",
        "  else:\n",
        "    dataset = re.sub(r'\\@\\w+|\\#\\w+','', dataset)\n",
        "    dataset = dataset.translate(str.maketrans('', '', (string.punctuation)))\n",
        "    \n",
        "  #Remove all stop-words\n",
        "  dataset = [word for word in str(dataset).split() if word not in stops]\n",
        "  dataset = (\" \").join(dataset)\n",
        "  \n",
        "  #Join all the words using ' ' before returning the tweet in the dataset\n",
        "  return str(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myQOu3KV4PZb"
      },
      "source": [
        "Use Function clean_data for data cleaning of both the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNwh-0BqCixq"
      },
      "outputs": [],
      "source": [
        "#Processing sentiment analysis dataset\n",
        "senti_df_cp['text'] = senti_df['text'].apply(lambda x: cleaned_data(x))\n",
        "\n",
        "#Processing canadian election dataset\n",
        "election_df_cp['text'] = election_df['text'].apply(lambda x: cleaned_data(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLT_d0Z0FGZZ"
      },
      "outputs": [],
      "source": [
        "senti_df_cp['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCqHjEclC9Do"
      },
      "outputs": [],
      "source": [
        "# Print sample of processed tweets\n",
        "print(senti_df_cp['text'][200])\n",
        "print(election_df_cp['text'][200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WBCeVHnIJHw"
      },
      "outputs": [],
      "source": [
        "election_df_cp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG6PmIABdnh7"
      },
      "source": [
        "#Section 2: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrPKU8v_eBhN"
      },
      "source": [
        "## **2.1 Steps for Assigning Tweets to a Party**\n",
        "\n",
        "To identify and link tweets to a political party following reference can be helpful\n",
        "\n",
        "1. Looking at the political parties and their leaders twitter handles e.g., @libral_party, @the jagmeetsingh can be reference and linked to the party. \n",
        "2. looking at party mentions in the tweets with hashtags e.g., #ForwardForEveryone, can be reference and linked to the party. \n",
        "3. identifying keywords realted to political parties and linkin them.\n",
        "\n",
        "\n",
        "Inorder to identify handles, hashtages and keywords from the tweets, visualtization is done by plotting wordclouds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfBBQ-PWenRg"
      },
      "source": [
        "**Analyzing Hashtags and Handles**\n",
        "\n",
        "The below cell defnies function for extracting handles and hashtags. These are then added to a new column respectively in the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnV0KxCAeO0f"
      },
      "outputs": [],
      "source": [
        "#function to find handles\n",
        "def find_handles(text_data):\n",
        "  text_data = re.findall('@\\w+', text_data)\n",
        "  return [re.sub('@','',x) for x in text_data]\n",
        "\n",
        "election_df_cp['handles'] = election_df_cp['text'].apply(lambda x: find_handles(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCVyXKW3e3Vo"
      },
      "outputs": [],
      "source": [
        "#function to find hashtags\n",
        "def find_hashtags(text_data):\n",
        "  text_data = re.findall('#\\w+', text_data)\n",
        "  return [re.sub('#','',x) for x in text_data]\n",
        "\n",
        "election_df_cp['hashtags'] = election_df_cp['text'].apply(lambda x: find_hashtags(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqxnak8r9k__"
      },
      "source": [
        "Wordclouds for extracted handles and hashtages stored in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kKIdJxzgLyy"
      },
      "outputs": [],
      "source": [
        "#Handles in the list format are exploded and joined for to plot wordcloud.\n",
        "handle_data = election_df_cp['handles'].explode().unique()\n",
        "\n",
        "handle_data_drop = ' '.join(np.delete(handle_data, 0))\n",
        "\n",
        "wordcloud = WordCloud(random_state=21, max_font_size=30,background_color ='lightblue').generate(handle_data_drop)\n",
        "\n",
        "#Plotting Wordcloud for handles\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('WordCloud for the Canadian Election handles')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJ-1BhbgG5Q"
      },
      "source": [
        "Some of the handles that are most frequent are: 'justintrudeau', 'thejagmeetsingh', 'erinotootle', 'codyotootle', 'cafreeland', 'maxinebernier', 'peoplespca' etc. These handles can be referenced with their affliation to a party.\n",
        "It will help in defining keywords for party allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R49fOGhbyMWB"
      },
      "outputs": [],
      "source": [
        "#Handles in the list format are exploded and joined for to plot wordcloud.\n",
        "hashtags_data = election_df_cp['hashtags'].explode().unique()\n",
        "\n",
        "hashtags_data_drop = ' '.join(np.delete(hashtags_data, 0))\n",
        "\n",
        "wordcloud = WordCloud(random_state=21, max_font_size=30, background_color ='lightblue').generate(hashtags_data_drop)\n",
        "\n",
        "#Plotting Wordcloud for handles\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('WordCloud for the Canadian Election hashtags')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzRvmwfvy2qa"
      },
      "source": [
        "It can be seen that hashtags like 'ndp', 'voteppc2021', 'ppc', 'purplereign', 'conservative','singh' etc. provide a good direction for the selection of different set of words assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_HiH7_XznNJ"
      },
      "source": [
        "##**2.2 Party Affiliation of a Tweet**\n",
        " \n",
        "Both the wordclouds helped to identify keywords relevant to a particular party. Only few keywords were listed in this study. Looking at the tweets and doing some research on the 2021 canadian elections, the tweets can be classified into Conservatives, Liberals,PPC and NDP based on commonly used hastags and names of politicians associated with different parties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlCfQ_ijzk_6"
      },
      "outputs": [],
      "source": [
        "#List of words that identify liberal\n",
        "liberal = ['liberal', 'liberalparty', 'forwardforeveryone','pm', 'government', \n",
        "           'lpc', 'justin', 'trudeau', 'justintrudeau', 'jt', 'cafreeland']\n",
        "\n",
        "#List of words that identify PPC\n",
        "PPC = ['ppc', 'peoplespartyofcanada', 'maximebernier', 'peoplespca', 'purple',\n",
        "       'peoplesparty', 'maxime', 'bernier', 'strongandfree' ]\n",
        "\n",
        "#List of words that identify NDP\n",
        "NDP = ['ndp', 'newdemocraticparty', 'newdemocratic', 'jagmeetsingh', 'fightingforyou'\n",
        "       'jagmeet', 'singh', 'thejagmeetsingh', 'initforyou' ]\n",
        "\n",
        "#List of words that identify CPC\n",
        "CPC = ['cpc_hq', 'conservative', 'conservativeparty', \n",
        "       'cpc', 'erin', 'otoole', 'erinotoole', 'codyotoole', 'kenney', 'jkenney']\n",
        "\n",
        "election_df_cp['party'] = 'None' ## initiating an empty colum in the dataframe to be filled with political parties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDCqXCziULpP"
      },
      "source": [
        "  *First all the handles and hashtages are removed and new cloumn is saved as 'clean text' before moving ahead in the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CzNub0QWZ2F"
      },
      "outputs": [],
      "source": [
        "#Cleaning canadian election data\n",
        "election_df_cp['clean_text'] = election_df_cp['text'].apply(lambda x: cleaned_data(x,rmove_handle_tag= True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7nlILCp_J6Y"
      },
      "source": [
        "Assigning tweets to a political party according to the keywords dictionay defined above. Only the unique mentions are assigned, in case there are multiple keywords for a particular tweet it is handled as 'Mixed' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUgPtrArYjuU"
      },
      "outputs": [],
      "source": [
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(liberal)),\"party\"]= \"Liberal\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(PPC)),\"party\"]= \"PPC\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(NDP)),\"party\"]= \"NDP\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(CPC)),\"party\"]= \"CPC\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqZEcAWphVkv"
      },
      "source": [
        "As mentioned above some tweets contain mentions of both the parties hence taking them into consideration as mixed tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ocyqvehgh6L"
      },
      "outputs": [],
      "source": [
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(liberal)) & election_df_cp['text'].str.contains('|'.join(CPC)),\"party\"] = \"Mixed\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(liberal)) & election_df_cp['text'].str.contains('|'.join(NDP)),\"party\"] = \"Mixed\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(liberal)) & election_df_cp['text'].str.contains('|'.join(PPC)),\"party\"] = \"Mixed\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(CPC)) & election_df_cp['text'].str.contains('|'.join(NDP)),\"party\"] = \"Mixed\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(CPC)) & election_df_cp['text'].str.contains('|'.join(PPC)),\"party\"] = \"Mixed\"\n",
        "election_df_cp.loc[election_df_cp['text'].str.contains('|'.join(NDP)) & election_df_cp['text'].str.contains('|'.join(PPC)),\"party\"] = \"Mixed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlhA3_BlAUKq"
      },
      "outputs": [],
      "source": [
        "election_df_cp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDdaS-7FALzc"
      },
      "outputs": [],
      "source": [
        "#Inspecting results of classification\n",
        "election_df_cp[[\"text\",\"sentiment\", \"party\"]].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jcn-_tNi69e"
      },
      "outputs": [],
      "source": [
        "election_df_cp[\"party\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAQBsGJ7oCqF"
      },
      "source": [
        "**The graph below shows the responses for each political party**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcWkGF3skJiZ"
      },
      "outputs": [],
      "source": [
        "##distribution of political parties \n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "total_entries = election_df_cp.shape[0]\n",
        "frequency_percent = [i/total_entries*100 for i in election_df_cp['party'].value_counts()]\n",
        "cum_frequency_percent = np.cumsum(frequency_percent) #calculation of cumulative value\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.yticks(np.arange(0,105,5), rotation='horizontal')\n",
        "plt.plot(election_df_cp['party'].value_counts().index.tolist(), cum_frequency_percent, 'r-o', \n",
        "         label='Cumulative frequency of political parties')\n",
        "plt.title(\"Cumulative Frequency of political parties\", fontsize = 15)\n",
        "plt.xlabel(\"Political Party\", fontsize = 15)\n",
        "plt.ylabel('Cumulative %', fontsize = 15)\n",
        "plt.legend()\n",
        "\n",
        "# exploring the profession of responders:\n",
        "plt.subplot(1,2,1)\n",
        "ax = election_df_cp['party'].value_counts().plot(kind='bar')\n",
        "ax.set_title('Political party distribution', fontsize = 15)\n",
        "ax.set_xlabel(\"Political Party\", fontsize = 15)\n",
        "ax.set_ylabel(\"Count of Response\", fontsize = 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjIMwNubomJo"
      },
      "source": [
        "From the above graph, it can be noted that the distribution of tweets are uneven. Libral party has the most count 298, the next indivial mentions is for the CPC party with 169 responses followed by PPC. NDP has the least meantionsof about 20. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5iZcJ-2pS6j"
      },
      "source": [
        "##**2.3 Canadian Election Data Visualization</font>**\n",
        "\n",
        "### **Canadian Election Dataset**\n",
        "\n",
        "One of the goals of this assignment is to identify the sentiment of the tweet from the canadian election dataset. There are two kinds of sentiments: 'negative' and 'positive'. \n",
        "\n",
        "To have a better understanding of both the sentiments, data is pivoted to sperate party and the sentiment for each of them.\n",
        "Below is the result of the pivot table that has the distribution of both sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ydb9thYlDjv"
      },
      "outputs": [],
      "source": [
        "##  using pivot tables\n",
        "election_df_pivot1 = election_df_cp[['sentiment', 'party']].copy()\n",
        "election_df_pivot1['count'] = 1\n",
        "\n",
        "## using pivot table to filter out desired columns\n",
        "election_df_pivot1 = pd.pivot_table(election_df_pivot1, values = 'count', index = 'party',columns = 'sentiment', aggfunc=np.sum, fill_value=0)\n",
        "election_df_pivot1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5s-cY--FeOa"
      },
      "source": [
        "To have a even better comparison, a stacked barplot is plotted below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2tmatud0xHE"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n",
        "sns.set(font_scale=1.2)\n",
        "# plt.subplot(2,1,1)\n",
        "ax = election_df_pivot1.loc[['Liberal','Mixed','None', 'CPC','NDP','PPC']].plot.bar(stacked=True, ax = ax1)\n",
        "plt.xticks(rotation='vertical', fontsize=\"10\")\n",
        "ax.set_title(\"Distribution of tweets with political parties and sentiments\", fontsize = 14)\n",
        "ax.set_xlabel(\"Political Parties\", fontsize = 14)\n",
        "ax.set_ylabel(\"Count of Tweets\", fontsize = 14)\n",
        "\n",
        "\n",
        "## Plots for Cumulative frequency of tweets for parties\n",
        "total_entries1 = election_df_cp.shape[0]\n",
        "frequency_percent1 = [i/total_entries1*100 for i in election_df_cp['party'].value_counts()]\n",
        "cum_frequency_percent1 = np.cumsum(frequency_percent1) #calculation of cumulative value\n",
        "\n",
        "plt.plot(election_df_cp['party'].value_counts().index.tolist(), cum_frequency_percent1, 'r-o', label='Cumulative frquency of Political Party Tweets')\n",
        "plt.xlabel('Political Parties', fontsize = 14)\n",
        "plt.ylabel('Cumulative Percentage', fontsize = 14)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title(\"Cumulative Frequency of political party tweets\", fontsize = 14)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "## Plots for distribution of negative tweets\n",
        "plt.figure(figsize = (8, 6), facecolor = None)\n",
        "ax = sns.histplot(data=election_df_cp, x='negative_reason', shrink=0.5,)\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right', fontsize='x-small')\n",
        "plt.ylabel(\"Count of Tweets\", fontsize = 14)\n",
        "plt.title('Distribution of Negative Reasons');\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1pXMVi9FupA"
      },
      "source": [
        "**Observation:**\n",
        "It is observed Liberal party has the most tweets count, but the number of negative sentiment tweets is higher than the positive sentiment tweets. While for rest of the groups positive sentiment is higher than the negative sentiment.\n",
        "NDP though having least count has only one negative tweet.\n",
        "\n",
        "To further analyse the breakup of negative sentiment, distribution is plotted that shows several categories or the reasons for negative sentiment. Few of the segnificant onces are others, calling early election and lies.\n",
        "This is further discussed in the last part of the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkqHQbE1D-AC"
      },
      "source": [
        "**Wordcloud for election sentiment** is employed to understand what kind of words are present in the negative and positive tweets. The below code separates the negative and positive tweet and then presents two plots one for each sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVNlQPU51ZWn"
      },
      "outputs": [],
      "source": [
        "## WordCloud for Positive tweets in Sentiments Dataset\n",
        "\n",
        "wordcloud_elec_pos = WordCloud(random_state=21, max_font_size=30, background_color ='lightgreen', colormap='Set1',\n",
        "                               min_font_size = 4).generate(str(election_df_cp[election_df_cp['sentiment']=='positive']['clean_text']))\n",
        "  \n",
        "# plot the WordCloud image                        \n",
        "plt.figure(figsize = (8, 6), facecolor = None) \n",
        "plt.imshow(wordcloud_elec_pos) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.title('Positive Sentiments wordcloud of Canadian Elections 2021', fontsize = 14, y = 50)\n",
        "plt.show()\n",
        "\n",
        "## WordCloud for NEGATIVE tweets in Sentiments Dataset\n",
        "\n",
        "wordcloud_elec_neg = WordCloud(random_state=21, max_font_size=30, background_color = 'salmon',\n",
        "                               min_font_size = 4).generate(str(election_df_cp[election_df_cp['sentiment']=='negative']['clean_text']))\n",
        "\n",
        "\n",
        "# plot the WordCloud image                        \n",
        "plt.figure(figsize = (8, 6), facecolor = None) \n",
        "plt.imshow(wordcloud_elec_neg) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.title('\\nNegative Sentiments wordcloud of Canadian Elections 2021', fontsize = 14, y = 50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwEmy0MuHuXG"
      },
      "source": [
        "**Observation:**\n",
        "1. Positive Word Cloud of Elections = Besides 'election' and 'vote' appearing in the positive word cloud, words like 'justintrudeau' and 'ppc' also appears in twitter. This suggested that public has a positive outlook for the liberal party. It can also be observed that there is a combination of both the positive and negative words.\n",
        "\n",
        "2. The negative wordcloud is more inclined towards negative words. Some of the words which can be highlighted from this plot are 'horrible', 'guns' and 'suffering'. Interestingly, it seems Justin Trudue (Liberal Party) and Otoole (Conservative Party) appear most in the negative cloud. Hence this could highlight that peoples hatred have been focused on the two major ruling parties in Canada i.e. Liberals and Conservatives\n",
        "\n",
        "\n",
        "3. There are a few generic and common words which could have been removed as a stopword such as 'Canadian', 'Election' and 'Canada'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlwRikdX9klf"
      },
      "source": [
        "## **2.4 Generic Sentiment Dataset**\n",
        "\n",
        "A similar approach is followed for the generic tweets. This dataset is going to be used as the training set for further steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dERV5UxN5eF4"
      },
      "outputs": [],
      "source": [
        "# Copy  of sentiment dataset to work on\n",
        "senti_df_cp_r = senti_df_cp.copy()\n",
        "senti_df_cp_r['text'] = senti_df['text'].apply(lambda x: cleaned_data(x,rmove_handle_tag=True))\n",
        "\n",
        "# Wordcloud for positive sentiment\n",
        "wordcloud_senti_pos = WordCloud(random_state=21, max_font_size=30, background_color ='lightgreen', colormap='Set1', \n",
        "                               min_font_size = 4).generate(str(senti_df_cp_r[senti_df_cp_r.label==1]['text']))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(wordcloud_senti_pos, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('WordCloud for generic positive tweets')\n",
        "plt.show()\n",
        "\n",
        "# Wordcloud for positive sentiment\n",
        "wordcloud_senti_neg = WordCloud(random_state=21, max_font_size=30, background_color ='salmon', \n",
        "                               min_font_size = 4).generate(str(senti_df_cp_r[senti_df_cp_r.label==0]['text']))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(wordcloud_senti_neg, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('\\nWordCloud for generic negative tweets')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMrVqyhEK9rp"
      },
      "source": [
        "**Observation:**\n",
        "The positive and negative cloud is presented above shows that there are  higher order of negative words are found in the negative plot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i4OY7qIIGw"
      },
      "source": [
        "# **Task 3: Sentiment Classification - Model implementation and tuning** \n",
        "\n",
        "This task is divided into 5 sections: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7NXVoucJYEq"
      },
      "source": [
        "## 3.a Machine learning models: (5 marks)\n",
        "Steps included are:\n",
        "\n",
        "1. Splitting the generic tweets into test and train dataset. 80% of the data is used as training set and the remaining data is considered as a test dataset.\n",
        "\n",
        "2. The original text is than vectorized using TF-IDF Vectorizer.\n",
        "\n",
        "3. The vectorizer is fitted using only the training data. The fitted vectorizer is later used to find the vectorized output for the other sets using the tranform option.\n",
        "4. 7 Machine learning Classifier models are trained to predict results on test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5J1EGo6KrV-"
      },
      "source": [
        "### 3.a.1 Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aPObOUAKk53"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset\n",
        "X = senti_df_cp_r\n",
        "y = senti_df_cp_r['label']\n",
        "\n",
        "#Splitting the dataset \n",
        "X_train_1, X_test, y_train_1, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=100)\n",
        "\n",
        "print(\"Train Set X, y: {},{}\".format(X_train_1.shape, y_train_1.shape))\n",
        "print(\"Test Set X, y: {}, {}\".format(X_test.shape, y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAI9OnAWXyJW"
      },
      "source": [
        "### 3.a.2 TF-DIF Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM33dGp5K9bd"
      },
      "source": [
        "While computing TF, all terms are considered equally important. However it is known that certain terms, such as “is”, “of”, and “that”, may appear a lot of times but have little importance. Thus it is need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
        "IDF is the inverse of the document frequency which measures the informativeness of term t.\n",
        "\n",
        "\n",
        "Source: https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpEt59cHK_GU"
      },
      "outputs": [],
      "source": [
        "#Defining the TFDIF Vectorizer\n",
        "vectorizer_tf = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None, max_features=5000)\n",
        "vectorizer_tf.fit(X_train_1['text'].values)    # fit data on training sample\n",
        "\n",
        "#Tranform the train and validation sets\n",
        "X_train_tf = vectorizer_tf.transform(X_train_1['text'].values)\n",
        "X_test_tf = vectorizer_tf.transform(X_test['text'].values)\n",
        "\n",
        "print(\"Shape after tfdif vectorizations\")\n",
        "print(\"Train Set X, y: {},{}\".format(X_train_tf.shape, y_train_1.shape))\n",
        "print(\"Test Set X, y: {}, {}\".format(X_test_tf.shape, y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amCw_b0_X9aY"
      },
      "source": [
        "### **3.a.3 Classifier Models</font>**\n",
        "\n",
        "This section focuses on using the prepared data as the input to the 7 different models with different classification algorithms: \n",
        "\n",
        "1. Logistic Regression\n",
        "2. Naive Bayes\n",
        "3. SVM\n",
        "4. Decision Tree\n",
        "5. Random Forest\n",
        "6. KNN\n",
        "7. XG-Boost\n",
        "\n",
        "The below cell implements a generalized function for each iteration of models runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYAr7hSHWhNS"
      },
      "outputs": [],
      "source": [
        "#function to implement model training\n",
        "def implement_model(model, X, y, Xtest, ytest, check=True):\n",
        "\n",
        "  X_trn = X\n",
        "  y_trn = y\n",
        "\n",
        "  #Model Fit\n",
        "  model.fit(X_trn,y_trn)\n",
        "\n",
        "  #Train Predictions\n",
        "  y_pred_trn = model.predict(X_trn)\n",
        "\n",
        "  #check this for the case when XGBClassifier is used.\n",
        "  if check==False:\n",
        "    y1 = [round(value) for value in y_pred_trn]\n",
        "    #y2 = [round(value) for value in y_pred_val]\n",
        "    y_pred_trn = y1\n",
        "    #y_pred_val = y2\n",
        "  \n",
        "  #Calculate Accuracies\n",
        "  acc_trn = metrics.accuracy_score(y_trn, y_pred_trn)\n",
        "\n",
        "  #Test Accuracies\n",
        "  y_preds = model.predict(Xtest)\n",
        "  acc_test = metrics.accuracy_score(y_test, y_preds)\n",
        "  report_test = classification_report(y_test,y_preds)\n",
        "\n",
        "  #Print Accuracies for comparison\n",
        "  print(\"The accuracy score for the Train set is: {0:.3}\".format(acc_trn))\n",
        "  print(\"The accuracy score for the Test set is: {0:.3}\".format(acc_test))\n",
        "  print(\"\\nThe classification report for the Test set is :\\n {}\".format(report_test))\n",
        "\n",
        "  return acc_trn, acc_test, report_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgLXjDUDYYGD"
      },
      "source": [
        "#### **Model 1: Logistic Regression**\n",
        "\n",
        "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n",
        "\n",
        "Source: https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc?gi=9e2a67956d00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D--tZ_J6GF72"
      },
      "outputs": [],
      "source": [
        "model_1 = LogisticRegression(max_iter=10000)\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for Logistic Regression model:\\n\")\n",
        "logistic_model = implement_model(model_1, X_train_tf, y_train_1, X_test_tf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLm-4N4rH-T9"
      },
      "source": [
        "####**Model 2: Naive Bayes (Multinomial)**\n",
        "\n",
        "nave Bayes classifiers are a type of \"probabilistic classifier\" based on Bayes' theorem and strong (nave) independence assumptions between features.\n",
        "Models for assigning class labels to problem instances that are represented as vectors of feature values, with the class labels taken from a finite set. For training such classifiers, there is no one algorithm, but rather a variety of algorithms based on the same principle: all naive Bayes classifiers assume that the value of one feature is independent of the value of any other feature, given the class variable. For example, if a fruit is red, round, and around 10 cm in diameter, it is termed an apple. Each of these characteristics is taken into account by a naive Bayes classifier, regardless of any possible correlations between the color, roundness, and diameter features.\n",
        "\n",
        "Source : https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyCNsL1VH9-F"
      },
      "outputs": [],
      "source": [
        "model_2 = MultinomialNB()\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for Naive Bayes (Multinomial) model:\\n\")\n",
        "naive_bayes_model = implement_model(model_2, X_train_tf, y_train_1, X_test_tf, y_test,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDvG8iyJAhZ"
      },
      "source": [
        "####**Model 3: SVM (Linear SVC)**\n",
        "LinearSVC uses a multi-class technique of \"one vs. the others\" to train n class models. Only one model is trained if there are only two classes. They are supervised learning models that evaluate data for classification and regression analysis, as well as associated learning algorithms. An SVM training algorithm creates a model that assigns new examples to one of two categories, making it a non-probabilistic binary linear classifier, given a series of training examples that are individually designated as belonging to one of two categories (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting).\n",
        "\n",
        "SVMs may perform non-linear classification as well as linear classification by implicitly mapping their inputs into high-dimensional feature spaces, which is known as the kernel trick.\n",
        "\n",
        "Source: https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3Z8L9z4Jb4w"
      },
      "outputs": [],
      "source": [
        "model_3 = LinearSVC(max_iter=10^6)\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for LinearSVC (SVM) model:\\n\")\n",
        "linear_SVC_model = implement_model(model_3, X_train_tf, y_train_1, X_test_tf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOfnPIB8K7NA"
      },
      "source": [
        "####**Model 4 Decision Trees**\n",
        "\n",
        "A decision tree is a decision-making aid that employs a tree-like model of decisions and their potential results, such as chance event outcomes, resource costs, and utility. It's one approach to show an algorithm made up entirely of conditional control statements.\n",
        "\n",
        "Decision trees are a popular technique in machine learning and are often used in operations research, specifically in decision analysis, to help determine the best method for achieving a goal.\n",
        "\n",
        "Source: https://www.datacamp.com/community/tutorials/decision-tree-classification-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmOlGXiWLGpO"
      },
      "outputs": [],
      "source": [
        "model_4 = DecisionTreeClassifier()\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for DecisionTree model:\\n\")\n",
        "decision_tree_model = implement_model(model_4, X_train_tf, y_train_1, X_test_tf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMBtX2feNTCS"
      },
      "source": [
        "####**Model 5 Random Forests**\n",
        "Random forests, also known as random decision forests, are an ensemble learning method for classification, regression, and other tasks that works by training a large number of decision trees and then outputing the class that is the mode of the classes (classification) or the mean prediction (regression) of the individual trees. Random decision forests address the problem of decision trees overfitting their training set.\n",
        "\n",
        "Source: https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x08SRZ7mQKh9"
      },
      "outputs": [],
      "source": [
        "model_5 = RandomForestClassifier()\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for Random Forest Classifier model:\\n\")\n",
        "RandomForest_model = implement_model(model_5, X_train_tf, y_train_1, X_test_tf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aEslOdQk00"
      },
      "source": [
        "#### **MODEL 6: k-Nearest Neighbours (kNN)Classifier**\n",
        "\n",
        "The outcome of k-NN classification is a class membership. An object is categorised by a majority vote of its neighbours, with the object allocated to the most common class among its k closest neighbours (k is a positive integer, typically small). If k = 1, the item is simply assigned to that single nearest neighbor's class.\n",
        "\n",
        "Source: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl4s-5VEQ_qJ"
      },
      "outputs": [],
      "source": [
        "model_6 = KNeighborsClassifier()\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for k-Nearest Neighbours Classifier model:\\n\")\n",
        "kNN_model = implement_model(model_6, X_train_tf, y_train_1, X_test_tf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hLMaQStR4rB"
      },
      "source": [
        "#### **MODEL 7: XGBosst Classifier**\n",
        "XGBoost is an implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning.\n",
        "\n",
        "Source: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSRUPUKeSRBp"
      },
      "outputs": [],
      "source": [
        "model_7 = XGBClassifier()\n",
        "\n",
        "print(\"\\nResults for Tf-idf Vectorizer for XGBosst Classifier model:\\n\")\n",
        "XGBClassifier_model = implement_model(model_7, X_train_tf, y_train_1, X_test_tf, y_test, check=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdyprW2QWugE"
      },
      "source": [
        "###3.a.4 Models Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2oOc9dyQelB"
      },
      "outputs": [],
      "source": [
        "#test cell\n",
        "fig, ax = plt.subplots(1,1,figsize=(15,8))\n",
        "\n",
        "for i in range(2):\n",
        "  if i == 0:\n",
        "    acc_trn_scores= [logistic_model[i], naive_bayes_model[i],linear_SVC_model[i],decision_tree_model[i],\n",
        "                   RandomForest_model[i],kNN_model[i],XGBClassifier_model[i]]\n",
        "  elif i == 1:\n",
        "    acc_test_scores= [logistic_model[i], naive_bayes_model[i],linear_SVC_model[i],decision_tree_model[i],\n",
        "                   RandomForest_model[i],kNN_model[i], XGBClassifier_model[i]] \n",
        "\n",
        "classifiers_list = ['Logistic regression', 'Naive Bayes', 'SVM', 'Decision Trees', 'Random Forests','KNN', 'XG Boost'] #\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "\n",
        "plt.plot(classifiers_list, acc_trn_scores, marker='o', color='y', linewidth=3, label = 'Accuracy train')\n",
        "\n",
        "plt.plot(classifiers_list, acc_test_scores, marker='o', color='b',linestyle='-.', linewidth=3, label = 'Accuracy test')\n",
        "\n",
        "plt.xticks(np.arange(0,7,1), rotation=0, fontsize=12)\n",
        "plt.xlabel('Classifier Models', fontsize=15)\n",
        "# plt.ylim([0.6,0.8])\n",
        "plt.ylabel('Accuracy Score', fontsize=15)\n",
        "plt.title('TFIDF Model Accuracies across classifiers')\n",
        "\n",
        "\n",
        "ymax = max(np.asarray(acc_trn_scores, dtype=np.float32))\n",
        "xmax = np.where(acc_trn_scores == np.amax(np.asarray(acc_trn_scores)))[0]\n",
        "plt.annotate('Max Acc. train = (%.5f)'%(np.amax(np.asarray(acc_trn_scores))), \n",
        "             xy=(xmax, ymax), xytext=(xmax-1.5, ymax+0.0005), \n",
        "             arrowprops=dict(facecolor='y', shrink = 0.005, frac=0.1)\n",
        "             )\n",
        "             \n",
        "ymax = max(np.asarray(acc_test_scores, dtype=np.float32))\n",
        "xmax = np.where(acc_test_scores == np.amax(np.asarray(acc_test_scores)))[0]\n",
        "plt.annotate('Max Acc. test = (%.5f)'%(np.amax(np.asarray(acc_test_scores))), \n",
        "             xy=(xmax, ymax), xytext=(xmax+0.5, ymax+0.010), \n",
        "             arrowprops=dict(facecolor='b', shrink = 0.005, frac=0.1)\n",
        "             )\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpTcKZ4KTNKV"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "It can be clearly seen from the above graph that the best model that priovided the best accuracy would be Logistic regression. The performance for that model on the test set is 0.965. Performance of two other models (Naive Bayes and SVM) are close to logistic regression.\n",
        "It is worthy to note that the decision tree classifers have best training accuracy by the test accuracy is not as good as training accuracy. On of the reason for this observation would be overfitting of the data. This is commonly observed in decision trees if the hyperparameters are not tuned. In this case, general model is used without hyperparameter tuning.\n",
        "XGboost had the least training and test accuracy.\n",
        "\n",
        "So, amongh all the models Logistic regression is preffered for this classification problem as it has th ebest train and test accuracy along with less computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ71otZNgtIL"
      },
      "source": [
        "## 3.b Deep learning model: (4 marks)\n",
        "\n",
        "Steps similar to above part is applied.\n",
        "1. Splitting the generic tweets into train (60%),validation(20%) and test(20%)dataset\n",
        "\n",
        "2. The original text is then vectorized using Bag of Words (TF).\n",
        "\n",
        "3. The vectorizer is fitted using only the training data. The fitted vectorizer is later used to find the vectorized output for the other sets using the tranform option.\n",
        "4. Deeplearing models are trained to predict results on validation and test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-WrePK5h9l9"
      },
      "source": [
        "### 3.b.1 Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUNv31idh9mR"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset further using split in part 3.a.1 in which data was split into Training 80% and Test 20%\n",
        "\n",
        "#Splitting the dataset \n",
        "X_train_2, X_val, y_train_2, y_val = train_test_split(X_train_1, y_train_1, test_size=0.25, stratify=y_train_1, random_state=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6AA_Qe-m-aD"
      },
      "source": [
        "###**3.b.2 Word Frequency Data Preparation (Bag of Words) or Term Frequency (TF)**\n",
        "\n",
        "Bag of Words is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set. \n",
        "\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
        "\n",
        "Source: https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EERbPQpRnDzY"
      },
      "outputs": [],
      "source": [
        "#Defining the Vectorizer\n",
        "vectorizer_bag = CountVectorizer(max_features = 2500)\n",
        "vectorizer_bag.fit(X_train_2['text'].values)\n",
        "\n",
        "#Transforming Data\n",
        "X_train_bag = vectorizer_bag.transform(X_train_2['text'].values)\n",
        "X_val_bag = vectorizer_bag.transform(X_val['text'].values)\n",
        "X_test_bag = vectorizer_bag.transform(X_test['text'].values)\n",
        "\n",
        "print(\"Shape after count vectorizations\")\n",
        "print(\"Train Set X, y: {},{}\".format(X_train_bag.shape, y_train_2.shape))\n",
        "print(\"Validation Set X, y: {},{}\".format(X_val_bag.shape, y_val.shape))\n",
        "print(\"Test Set X, y: {}, {}\".format(X_test_bag.shape, y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8KVo4_eqE7H"
      },
      "source": [
        "### **3.b.3 Deeplearning Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD5T0UiZcyGZ"
      },
      "source": [
        "#### Deeplearning MPL classifier model\n",
        "\n",
        "MLPClassifier stands for Multi-layer Perceptron Classifier, which is linked to a Neural Network by its name. Unlike other classification methods such as Support Vectors or Naive Bayes Classifier, MLPClassifier does classification using an underlying Neural Network.\n",
        "\n",
        "However, MLPClassifier is identical to Scikit-other Learn's classification algorithms in that it requires no more effort to implement than Support Vectors, Naive Bayes, or any other Scikit-Learn classifier. \n",
        "\n",
        "Although, the MLPClassifier using the hidden Neural network make the increases computation time.\n",
        "\n",
        "In order limit the time, only 2500 features is selected for the model. This might led to lesser accuracy in the model but is adopted in this assignment, due to computaion limitation. \n",
        "\n",
        "Source : https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWSAVeiPs0Rj"
      },
      "outputs": [],
      "source": [
        "clf_model = MLPClassifier(hidden_layer_sizes = 100, batch_size=512, \n",
        "                          learning_rate_init = 0.01, shuffle = True, \n",
        "                          verbose = False, random_state=1, max_iter=50\n",
        "                          ).fit(X_train_bag, y_train_2)\n",
        "\n",
        "#Model Fit                          \n",
        "clf_model.fit(X_train_bag, y_train_2)\n",
        "\n",
        "#Train Predictions\n",
        "y_pred_trn_1 = clf_model.predict(X_train_bag)\n",
        "\n",
        "#Validation Predictions\n",
        "y_val_pred_1 = clf_model.predict(X_val_bag)\n",
        "\n",
        "#Calculate Accuracies\n",
        "acc_train_1 = metrics.accuracy_score(y_train_2, y_pred_trn_1)\n",
        "acc_val_1 = metrics.accuracy_score(y_val, y_val_pred_1)\n",
        "\n",
        "print('The training accuracy without hyperparameter tuning is: ', acc_train_1)\n",
        "print('The validation accuracy without hyperparameter tuning is: ', acc_val_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrVDKzOTX8uT"
      },
      "source": [
        "Results from the above model is comparable with the ML models implemented earlier with Training accuracy (0.993) and test accuracy (0.952).\n",
        "\n",
        "In the next step, hyperparameter is tuned usinig grid search.\n",
        "Here, two parameter hidden_layer_sizes is tuned along with the learning rate.\n",
        "\n",
        "Few other parameters that can be trained include the activation function and also the level of neural networks. It is not adpoted in this assignment due to long running time and heavy computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exom7wXSzwA2"
      },
      "outputs": [],
      "source": [
        "# Hypertuning parameters using gridsearch\n",
        "param = {'hidden_layer_sizes': [30,50,100],\n",
        "         'learning_rate_init':[0.1, 0.01],\n",
        "         #'activation': ['tanh', 'relu'],          # Removed due to heavy computation\n",
        "         }\n",
        "         \n",
        "MLP_model = MLPClassifier(batch_size= 1024,max_iter= 50, random_state=128)\n",
        "MLP_model_gridsearch = GridSearchCV(MLP_model, param, verbose=0, scoring='accuracy')\n",
        "\n",
        "#Model Fit\n",
        "MLP_model_gridsearch.fit(X_train_bag, y_train_2)\n",
        "\n",
        "#Train Predictions\n",
        "y_pred_trn_2 = MLP_model_gridsearch.predict(X_train_bag)\n",
        "\n",
        "#Validation Predictions\n",
        "y_val_pred_2 = MLP_model_gridsearch.predict(X_val_bag)\n",
        "\n",
        "#Calculate Accuracies\n",
        "acc_train_2 = metrics.accuracy_score(y_train_2, y_pred_trn_2)\n",
        "acc_val_2 = metrics.accuracy_score(y_val, y_val_pred_2)\n",
        "\n",
        "#Best model\n",
        "MLP_model_gridsearch_bs = MLP_model_gridsearch.best_score_\n",
        "MLP_model_gridsearch_bp = MLP_model_gridsearch.best_params_\n",
        "\n",
        "print('The training accuracy hyperparameter tuning is: ', acc_train_2)\n",
        "print('The validation accuracy hyperparameter tuning is: ', acc_val_2)\n",
        "print('\\n Best Tuning Score is =', MLP_model_gridsearch_bs)\n",
        "print('\\n Best Parameters are =', MLP_model_gridsearch_bp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb3ZTbgoZNS3"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "Above are the results from gridsearch : best parameters are {'hidden_layer_sizes': 100, 'learning_rate_init': 0.01} with test accuracy of (0.954)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77CUBwnEJWez"
      },
      "source": [
        "#### **Deeplearning Nerural network with Pytorch**\n",
        "\n",
        "The deep learning model is implemented with Pytorch.\n",
        "Steps involved:\n",
        "1. X_train, X_val  and X_test data that were transformed using Countvectorizer is now converted to torch.tensor, that be used as an input for the model.\n",
        "\n",
        "2. All the X_data Data is loaded using DataLoader as an input to the neural network.\n",
        "3. Model parameters are defined with loss function, epoch and learning rate.\n",
        "4. Model training\n",
        "5. Prediction on y_val.\n",
        "6. Hyperparameter training for best model parameters.\n",
        "7. Preaction on testset with the best model parameters.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpBchDkyJkrs"
      },
      "outputs": [],
      "source": [
        "# Define a function to convert x_csr matrix to torch matrix\n",
        "def convert_torch(X_data):\n",
        "\n",
        "  coo = coo_matrix((X_data), shape=(X_data.shape[0], X_data.shape[1]))\n",
        "\n",
        "  values = coo.data\n",
        "  indices = np.vstack((coo.row, coo.col))\n",
        "\n",
        "  i = torch.LongTensor(indices)\n",
        "  v = torch.FloatTensor(values)\n",
        "  shape = coo.shape\n",
        "\n",
        "  X_data_torch = torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()\n",
        "\n",
        "  return X_data_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tEgYRb1Jwlj"
      },
      "outputs": [],
      "source": [
        "X_train_bag_torch = convert_torch(X_train_bag) # Convert train data to torch\n",
        "X_val_bag_torch = convert_torch(X_val_bag)     # Convert val data to torch\n",
        "X_test_bag_torch = convert_torch(X_test_bag)   # Convert train data to torch\n",
        "\n",
        "type(X_train_bag_torch),type(X_val_bag_torch),type(X_test_bag_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIrkrzF6OKaH"
      },
      "source": [
        "### Modeling : **Deep Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqpWSJURORLm"
      },
      "outputs": [],
      "source": [
        "#defining dataset class\n",
        "\n",
        "class train_val_data(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "train_torch = train_val_data(X_train_bag_torch , torch.FloatTensor(y_train_2.to_numpy()))\n",
        "val_torch = train_val_data(X_val_bag_torch , torch.FloatTensor(y_val.to_numpy()))\n",
        "\n",
        "## test data    \n",
        "class test_data(Dataset):\n",
        "    \n",
        "    def __init__(self, x):\n",
        "      self.x = x\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x)\n",
        "    \n",
        "test_torch = test_data(X_test_bag_torch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w30bYzNfP0Ld"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "epochs = 10\n",
        "b_size = 160\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSiKV4ldPv8H"
      },
      "outputs": [],
      "source": [
        "#DataLoader\n",
        "train_loader = DataLoader(train_torch, batch_size= b_size)\n",
        "val_loader = DataLoader(val_torch, batch_size=b_size)\n",
        "test_loader = DataLoader(test_torch, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9uHvrbQJPZ"
      },
      "outputs": [],
      "source": [
        "#Define the neural network\n",
        "\n",
        "class classification(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(classification,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_shape,64)\n",
        "    self.fc2 = nn.Linear(64,32)\n",
        "    self.fc3 = nn.Linear(32,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeTuUtYQ1tL"
      },
      "outputs": [],
      "source": [
        "# nnModel \n",
        "model_nn = classification(input_shape= 2500)\n",
        "optimizer = torch.optim.SGD(model_nn.parameters(),lr=learning_rate)\n",
        "loss_function = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXwRSkYgRIiL"
      },
      "outputs": [],
      "source": [
        "# Define accuracy function\n",
        "def accuracy(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPHbvYefRiI7"
      },
      "outputs": [],
      "source": [
        "# Model training \n",
        "model_nn.train()\n",
        "for epoch in tqdm(range(0, epochs)):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model_nn(X_batch)\n",
        "        \n",
        "        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = accuracy(y_pred, y_batch.unsqueeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    print(f'Epoch {epoch+ 1}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AldWyy7SnPr"
      },
      "outputs": [],
      "source": [
        "# Prediction on validation data\n",
        "list_y_pred = []\n",
        "model_nn.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch,y_batch in val_loader:\n",
        "        y_val_pred = model_nn(X_batch)\n",
        "        y_val_pred = torch.sigmoid(y_val_pred)\n",
        "        y_pred_tag = torch.round(y_val_pred)\n",
        "        list_y_pred.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "list_y_pred = [a.squeeze().tolist() for a in list_y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmrhnzOITGqi"
      },
      "outputs": [],
      "source": [
        "y_pred_list_temp = []\n",
        "for i in list_y_pred:\n",
        "  for elements in i:\n",
        "    y_pred_list_temp.append(elements)\n",
        "y_pred_list = y_pred_list_temp\n",
        "report_nn = classification_report(y_val, y_pred_list)\n",
        "print(\"\\nThe classification report for the Validation set is :\\n {}\".format(report_nn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0o30scKT8Um"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter tuning\n",
        "best_params_hp = {}\n",
        "best_accuracy_hp = 0\n",
        "\n",
        "\n",
        "for learning_rate_hp in tqdm([0.1,0.01,0.001]):\n",
        "\n",
        "    for b_size_hp in [32,64,128,256]:\n",
        "\n",
        "      train_loader_hp = DataLoader(train_torch, batch_size= b_size_hp)\n",
        "      val_loader_hp = DataLoader(val_torch, batch_size = b_size_hp)\n",
        "      \n",
        "      model = classification(input_shape= 2500)\n",
        "      optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate_hp)\n",
        "      loss_fn = nn.BCEWithLogitsLoss() \n",
        "\n",
        "      model.train()\n",
        "      for X_batch, y_batch in train_loader_hp:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred_hp = model(X_batch)\n",
        "        \n",
        "        loss = loss_fn(y_pred_hp, y_batch.unsqueeze(1))\n",
        "        acc = accuracy(y_pred_hp, y_batch.unsqueeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "      y_pred_list_hp = []\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          for X_batch,y_batch in val_loader_hp:\n",
        "             \n",
        "              y_val_pred_hp = model(X_batch)\n",
        "              y_val_pred_hp = torch.sigmoid(y_val_pred_hp)\n",
        "              y_pred_tag_hp = torch.round(y_val_pred_hp)\n",
        "              y_pred_list_hp.append(y_pred_tag_hp.cpu().numpy())\n",
        "\n",
        "      y_pred_list_hp = [a.squeeze().tolist() for a in y_pred_list_hp]\n",
        "      \n",
        "      y_pred_list_temp_hp = []\n",
        "      for i in y_pred_list_hp:\n",
        "        for element_hp in i:\n",
        "          y_pred_list_temp_hp.append(element_hp)\n",
        "      y_pred_list_new_hp = y_pred_list_temp_hp\n",
        "\n",
        "      con_matrix_hp = confusion_matrix(y_val, y_pred_list_new_hp)\n",
        "      #res = cm.tolist()\n",
        "      accuracy_nn_hp = (con_matrix_hp[0][0] + con_matrix_hp[1][1] ) * 100/ (con_matrix_hp[0][0] + con_matrix_hp[1][0] + con_matrix_hp[0][1] + con_matrix_hp[1][1] )\n",
        "      print(\"Accuracy for learning rate:\",learning_rate_hp, ' and batch size:',b_size_hp,' is:', accuracy_nn_hp)\n",
        "\n",
        "      if accuracy_nn_hp > best_accuracy_hp:\n",
        "        best_params_hp = {'Learning rate':learning_rate_hp, 'Batch size':b_size_hp, 'model' : model}\n",
        "        best_accuracy_hp = accuracy_nn_hp\n",
        "\n",
        "print ('\\n','Best Parameters are:','\\n',best_params_hp)\n",
        "\n",
        "print ('\\n',\"Best Score: {0:.3}\".format(best_accuracy_hp))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW4b_IVbVv0t"
      },
      "outputs": [],
      "source": [
        "#Testing model using tuned parameters\n",
        "\n",
        "y_pred_test_list = []\n",
        "model = best_params_hp.get('model')\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "\n",
        "        y_test_pred = model(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_test_list.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "y_pred_test_list = [a.squeeze().tolist() for a in y_pred_test_list]\n",
        "\n",
        "#Test Accuracies\n",
        "acc_test_nn = metrics.accuracy_score(y_test, y_pred_test_list)\n",
        "report_test_nn = classification_report(y_test,y_pred_test_list)\n",
        "\n",
        "#Print Accuracies for comparison\n",
        "print(\"The accuracy score for the Test set is: {0:.3}\".format(acc_test_nn))\n",
        "print(\"\\nThe classification report for the Test set is :\\n {}\".format(report_test_nn))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srnQYVdf75m"
      },
      "source": [
        "## **3.c Implementing Final Model on elections dataset: (0.5 marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grWRhYgXgaTv"
      },
      "source": [
        "Based on the performance of all the models in part 3.a and 3.b, logistics regression performed the best. Deep learning model also shows good results, inorder to keep the computation time in check, logistic regression model is selected for prediction on Canadian election dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2Stnc7Wg0aJ"
      },
      "outputs": [],
      "source": [
        "## Mapping positive and negative to 1 and 0 in the Canadian elections dataset\n",
        "map_sentiment = {'positive':1, 'negative':0}\n",
        "\n",
        "X_election = election_df_cp['clean_text']\n",
        "y_election = election_df_cp.replace({'sentiment': map_sentiment})['sentiment']\n",
        "\n",
        "X_election_tf = vectorizer_tf.transform(X_election)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OV03JMqiyWK"
      },
      "outputs": [],
      "source": [
        "#Best model Logistic Regression is used to predict the election results\n",
        "\n",
        "model_final = LogisticRegression(max_iter=10000)\n",
        "\n",
        "#Model Fit\n",
        "model_final.fit(X_train_tf,y_train_1)\n",
        "\n",
        "#Test Predictions\n",
        "y_pred_test = model_final.predict(X_test_tf)\n",
        "\n",
        "#Calculate Accuracies\n",
        "acc_test_model = metrics.accuracy_score(y_test, y_pred_test)\n",
        "  \n",
        "print(\"The accuracy score for the Test (Generic dataset) is: {}\".format(acc_test_model))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EocQUdO30sIN"
      },
      "outputs": [],
      "source": [
        "#Test and prediction Accuracies on Canadian Election dataset\n",
        "y_preds_elect = model_final.predict(X_election_tf)  ## Using this for further prediction\n",
        "\n",
        "acc_test_elect = metrics.accuracy_score(y_election, y_preds_elect)\n",
        "report_test_elect = classification_report(y_election,y_preds_elect)\n",
        "\n",
        "#Print Accuracies for comparison\n",
        "print(\"The accuracy score for the Test (Election Dataset) is: {0:.3}\".format(acc_test_elect))\n",
        "print(\"\\nThe classification report for the Test (Election Dataset) is :\\n {}\".format(report_test_elect))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HI5gOX5hx3Q"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "Logisticregression model achieved accuracy of 0.714. Accuracy drop is clear as compared with the generic dataset where test accuracy was 0.966.\n",
        "There could be multiple reasons for lower accuracy, one being the features in both the datasets.\n",
        "\n",
        "As observed in the wordclouds for negative election datasets many keywords related to political parties was seen e.g, 'trudue' (Liberal Party) and 'otoole' (Conservative Party). These keywords have very less probalilty occurances in the generic dataset used to train the model. Same is applicable with the positive sentiment keywords in both the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gptbQLn2mo2_"
      },
      "source": [
        "## **3.d Other evaluation metrics**\n",
        "\n",
        "Evalution metric used so far is accuracy_score [sum of diagonal elements (true positive and true negative)/ all elements] of the confusion matrix. Since, the assignment deals with binary classification of positive and negative sentiments is a good matrix for evalution considering distribution of the dataset roughly (2/3 positive and 1/3 negative).\n",
        "Incase dataset is even more unbalanced two matrix that could be applied are.\n",
        "1. F1 score: This matrix is best for highly imbalanced data, it involves presion and recall to compute the F1 score, which ensures better handling of misclassification.\n",
        "**Note**: These scores are already plotted with the results of the model as classification, but the model selection is done based on accuracy_score\n",
        "\n",
        "2. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:True Positive Rate, False Positive Rate\n",
        "It an graphical representation to have comparision among different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auv18EfOna1i"
      },
      "source": [
        "## **3.e Visualize the sentiment prediction results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XobVsk9jn-Oo"
      },
      "outputs": [],
      "source": [
        "## Making a Dataframe of True and False prediction on Canadian elections dataset\n",
        "import copy\n",
        "election_df_cp_2 = copy.deepcopy(election_df_cp)\n",
        "election_df_cp_2 = election_df_cp_2.drop(columns = ['handles','hashtags'])\n",
        "election_df_cp_2['sentiment_predictions'] = pd.Series(y_preds_elect).map({0:'negative',1:'positive'})\n",
        "\n",
        "election_df_cp_2['sentiment_vs_pred'] = 0\n",
        "\n",
        "for i in range (election_df_cp_2.shape[0]):\n",
        "    if election_df_cp_2['sentiment'][i] == election_df_cp_2['sentiment_predictions'][i]:\n",
        "        election_df_cp_2['sentiment_vs_pred'][i] = 'Correct_Prediction'\n",
        "    else:\n",
        "        election_df_cp_2['sentiment_vs_pred'][i] = 'Wrong_Prediction'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL5rpWbHo68r"
      },
      "outputs": [],
      "source": [
        "election_df_cp_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qx5zF5Rq5ul"
      },
      "outputs": [],
      "source": [
        "#Plot Correct vs Wrong Predictions for each party \n",
        "plt.figure(figsize=(12,7))\n",
        "party_order = ['Liberal','Mixed','None', 'CPC','NDP','PPC']\n",
        "ax = sns.countplot(x=\"party\", data=election_df_cp_2, hue='sentiment_vs_pred', order= party_order)\n",
        "ax.set_title('Correct vs Wrong Predictions for each party', fontsize = 20)\n",
        "ax.set_xlabel('Political Party', fontsize = 17)\n",
        "ax.set_ylabel('Count of Predictions', fontsize = 17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hl8imEEtWja"
      },
      "outputs": [],
      "source": [
        "#Plot \n",
        "plt.figure(figsize=(20,4*2))\n",
        "gs = gridspec.GridSpec(1,2)\n",
        "\n",
        "plt.subplot(gs[0])\n",
        "ax1 = sns.countplot(x=\"party\", data=election_df_cp_2[election_df_cp_2['sentiment_vs_pred'] == 'Correct_Prediction'], hue='sentiment_predictions',order= party_order)\n",
        "ax1.set_title('Correct Sentiment Predictions Political Distribution', fontsize = 20)\n",
        "ax1.set_xlabel('Political Party', fontsize = 15)\n",
        "ax1.set_ylabel('Count of Predictions', fontsize = 15)\n",
        "\n",
        "\n",
        "plt.subplot(gs[1])\n",
        "ax2 = sns.countplot(x=\"party\", data=election_df_cp_2, hue='sentiment_predictions',order= party_order)\n",
        "ax2.set_title('Overall Sentiment Predictions Political Distribution', fontsize = 20)\n",
        "ax2.set_xlabel('Political Party', fontsize = 15)\n",
        "ax2.set_ylabel('Count of Predictions', fontsize = 15)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "plt.tight_layout()\n",
        "\n",
        "display(Markdown('\\n---------------------------------- Model Result ----------------------------------\\n'))\n",
        "print('Accuracy = ', \"{0:.3%}\".format(acc_test_elect))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umWkiJ9ODmGz"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "It is evident from the graphs that three political parties and the mixed set have quite a lot negative sentiments expressed in tweets. \n",
        "\n",
        "Top two parties **Liberal Party** and **Conservative party** have higher proportion of negative sentiments in overall prediction, but the conservative party correct prediction is more or less equal as compared to overall prediction. \n",
        "\n",
        "Moreover, the **Liberal Party**, although the party has many negetive tweet, the predicted overall positive to negtive tweet ratio is high. Also, Liberal Party has the maximum number of predicted positive tweets amongst all three political parties.\n",
        "\n",
        "Additionally, **NDP** has the best predicted positive to negative tweets ratio amongst all the three political parties. But,the total number of twwet for the NDP is quite low to produce any quantifiable and measurbale impact to the predictions\n",
        "\n",
        "**Model Predicted Result:**\n",
        "\n",
        "According to the anticipated model outlined in the previous section, the **Liberal Party** should win.\n",
        "\n",
        "**Actual Election Result 2021:**\n",
        "\n",
        "The **Liberal Party** won the elections.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "This demonstrates that tweet **NLP analytics can be a highly effective tool** for predicting election outcomes. The public opinion regarding political parties is represented through tweets, and performing tweet NLP can assist political parties in assessing public sentiment and making appropriate reforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH0DY06DDn15"
      },
      "source": [
        "# **Task 4: Negative Reason Classification - Model implementation and tuning (3 marks):** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDb7KjewxNv"
      },
      "source": [
        "The focus of this section is on the negative labelled data in the Canadian election dataset. The dataset is then divided into 70 % for training and 30 % for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNPPlsYiGgiG"
      },
      "outputs": [],
      "source": [
        "election_df_cp_2_negative = election_df_cp_2[election_df_cp_2['sentiment']=='negative'][['negative_reason', 'clean_text']]\n",
        "election_df_cp_2_negative.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYlOhzedL2vQ"
      },
      "outputs": [],
      "source": [
        "election_df_cp_2_negative['negative_reason'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoFdVb6NLKws"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "\n",
        "ax3 = sns.countplot(x=\"negative_reason\", data=election_df_cp_2_negative, order = election_df_cp_2_negative['negative_reason'].value_counts().index)\n",
        "plt.xticks(rotation='vertical', fontsize=\"15\")\n",
        "ax3.set_xlabel('Negative Reason', fontsize = '15')\n",
        "ax3.set_ylabel('Count', fontsize = '20')\n",
        "ax3.set_title('Count of Negative Reasons with labels as percentage of total',  fontsize = '20')\n",
        "\n",
        "for p, label in zip(ax3.patches, np.round(np.true_divide(election_df_cp_2_negative['negative_reason'].value_counts().values, election_df_cp_2_negative['negative_reason'].value_counts().sum()),3)):\n",
        "    ax3.annotate(\"{0:.1%}\".format(label), (p.get_x()+0.15, p.get_height()+0.4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1acFHbMLuIw"
      },
      "source": [
        "Observation:\n",
        "It can be observed from the table above that the data is skewed for the last 5 categories which represent a very low percentage of the total. Hence we can combine a few categories\n",
        "\n",
        "Categories =\n",
        "\n",
        "1. Calling election early\n",
        "\n",
        "2. Exploitation = 'Tell Lies' + 'Scandal' + 'segregation' - Reason being : They all form a part of manipultion by leaders\n",
        "\n",
        "3. Economy = 'economy' + 'covid' - Reason being : COVID has officially become a reason for downfall of ecomomy\n",
        "\n",
        "4. Healthcare = 'Women Reproductive right and Racism' + 'Healthcare and Marijuana' - Reason being : They all relate to healthcare and wellness of the public\n",
        "\n",
        "5. Gun control\n",
        "\n",
        "6. Climate Problem\n",
        "\n",
        "7. Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtoaFsU3MABg"
      },
      "outputs": [],
      "source": [
        "election_df_cp_2_neg_comb = copy.deepcopy(election_df_cp_2_negative)\n",
        "\n",
        "election_df_cp_2_neg_comb['combined_neg_reason'] = election_df_cp_2_neg_comb['negative_reason'].map({'tell lies':'Exploitation',\n",
        "                                                                                                     'calling election early':'early election',\n",
        "                                                                                                      'scandal':'Exploitation',\n",
        "                                                                                                       'segregation':'Exploitation',\n",
        "                                                                                                        'economy':'Economy',\n",
        "                                                                                                        'covid ':'Economy',      \n",
        "                                                                                                        'women reproductive right and racism':'Healthcare',\n",
        "                                                                                                         'healthcare and marijuana':'Healthcare',\n",
        "                                                                                                          'climate problem':'Climate Problem', \n",
        "                                                                                                           'gun control':'gun control',\n",
        "                                                                                                           'others':'Others'\n",
        "                                                                                                      }\n",
        "                                                                                                    )\n",
        "election_df_cp_2_neg_comb.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfYmYw5_QMqo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "ax4 = sns.countplot(x=\"combined_neg_reason\", data=election_df_cp_2_neg_comb, order = election_df_cp_2_neg_comb['combined_neg_reason'].value_counts().index)\n",
        "plt.xticks(rotation='vertical', fontsize=\"15\")\n",
        "ax4.set_xlabel('Negative Reason Combined', fontsize = '20')\n",
        "ax4.set_ylabel('Count', fontsize = '20')\n",
        "ax4.set_title('Count of COMBINED Negative Reasons with labels as percentage of total',  fontsize = '20')\n",
        "\n",
        "for p, label in zip(ax4.patches, np.round(np.true_divide(election_df_cp_2_neg_comb['combined_neg_reason'].value_counts().values, election_df_cp_2_neg_comb['combined_neg_reason'].value_counts().sum()),3)):\n",
        "    ax4.annotate(\"{0:.1%}\".format(label), (p.get_x()+0.15, p.get_height()+1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1zOBFlQ1J3"
      },
      "source": [
        "**Observation:** The data looks much better now and can be further processed for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaQZYVQVQ3RY"
      },
      "source": [
        "Implimenting classification models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl5nIWAwRVch"
      },
      "outputs": [],
      "source": [
        "## Encoding for multiclass classification\n",
        "neg_mapping = {'Others':0, 'Exploitation':1, 'early election':2, 'Economy':3,'gun control':4,'Climate Problem':5,'Healthcare':6}\n",
        "\n",
        "X_elec_comb = election_df_cp_2_neg_comb['clean_text']\n",
        "y_elec_comb = election_df_cp_2_neg_comb.replace({'combined_neg_reason': neg_mapping})['combined_neg_reason']\n",
        "\n",
        "X_elec_comb_tr, X_elec_comb_test, y_elec_comb_tr, y_elec_comb_test = train_test_split(X_elec_comb, y_elec_comb, test_size=0.30, stratify=y_elec_comb, random_state=100)\n",
        "\n",
        "#Use countVectorizer()\n",
        "vectorizer_tf_2 = CountVectorizer()\n",
        "vectorizer_tf_2.fit(X_elec_comb)\n",
        "\n",
        "#Transform the test and the train data using the fitted vectorizer\n",
        "X_elec_comb_tr_tf = vectorizer_tf_2.transform(X_elec_comb_tr)\n",
        "X_elec_comb_test_tf = vectorizer_tf_2.transform(X_elec_comb_test)\n",
        "\n",
        "print(\"Shape after count vectorizations\")\n",
        "print(\"Train Set X, y: {},{}\".format(X_elec_comb_tr_tf.shape, y_elec_comb_tr.shape))\n",
        "print(\"Test Set X, y: {}, {}\\n\".format(X_elec_comb_test_tf.shape, y_elec_comb_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhpEYknr-GrK"
      },
      "outputs": [],
      "source": [
        "def implement_model_cv(model, X, y, params, scoring):\n",
        "  \n",
        "  param_grid = params\n",
        "\n",
        "  kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "  clf_grid = GridSearchCV(model, param_grid, scoring=scoring, cv=kfold, return_train_score=True)\n",
        "  clf_grid.fit(X, y)\n",
        "\n",
        "  results = pd.DataFrame.from_dict(clf_grid.cv_results_)\n",
        "  results = results.sort_values(['param_{}'.format(list(params.keys())[0])])\n",
        "  print(\"The parameter is: {}\".format(['param_{}'.format(list(params.keys())[0])]))\n",
        "  \n",
        "  train_score_mean = results['mean_train_score']\n",
        "  train_score_std = results['std_train_score']\n",
        "\n",
        "  cv_score_mean = results['mean_test_score']\n",
        "  cv_score_std = results['std_test_score']\n",
        "\n",
        "  K = results['param_{}'.format(list(params.keys())[0])]\n",
        "  plt.plot(K, train_score_mean, label='Train Score')\n",
        "  plt.plot(K, cv_score_mean, label='CV Score')\n",
        "\n",
        "  plt.scatter(K, train_score_mean, label='Train Score points')\n",
        "  plt.scatter(K, cv_score_mean, label='CV Score points')\n",
        "\n",
        "  plt.xscale('log')\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"K: hyperparameter\")\n",
        "  plt.ylabel(\"Score\")\n",
        "\n",
        "  plt.title(\"Hyper parameter Vs Accuracy Score plot\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  error_train = np.add(np.power((1-train_score_mean),2),np.power(train_score_std,2))\n",
        "  error_test = np.add(np.power((1-cv_score_mean),2),np.power(cv_score_std,2))\n",
        "\n",
        "  plt.plot(K, error_train, label='Train Total Error')\n",
        "  plt.plot(K, error_test, label='CV Total Error')\n",
        "\n",
        "  plt.scatter(K, error_train, label='Train Error points')\n",
        "  plt.scatter(K, error_test, label='CV Error points')\n",
        "\n",
        "  plt.xscale('log')\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"K: hyperparameter\")\n",
        "  plt.ylabel(\"Error\")\n",
        "\n",
        "  plt.title(\"Hyper parameter Vs Total Error (Bias^2 + Variance)\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\nThe best parameter value is: {}\".format(clf_grid.best_params_))\n",
        "  print(\"\\nThe best score is: {}\".format(clf_grid.best_score_))\n",
        "\n",
        "  return clf_grid.best_score_, clf_grid.best_params_, clf_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znuhh_Y7_YcM"
      },
      "source": [
        "### **Selected Model: Logisitic Regression**\n",
        "\n",
        "Logistic Regression was chosen for this challenge just to compare with the results from the previous section. The overall error is lowest for the C value of 1, as can be seen from the charts below. Despite the fact that the model's performance on the test set is far inferior to that on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAFDb91S_TOX"
      },
      "outputs": [],
      "source": [
        "model_election_neg = LogisticRegression(max_iter=10000, multi_class='multinomial')\n",
        "C = [0.01, 1, 100, 1000, 10000, 100000]\n",
        "param_grid = dict(C=C)\n",
        "\n",
        "print(\"\\nResults when Count Vectorizer input is selected:\\n\")\n",
        "best_score, best_param, best_est = implement_model_cv(model_election_neg, X_elec_comb_tr_tf, y_elec_comb_tr, param_grid, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44j4-pnGDHoU"
      },
      "source": [
        "The final model's hyperparameters are selected set, and the model's performance is evaluated across the entire training set. As can be seen, the model performs admirably. This indicates that there is overfitting in this instance. The dataset is quite small, which makes it difficult to improve the test results if the same dataset is used.\n",
        "\n",
        "We can see from the hyperparameter tuning section that the cross validation set's accuracy was very low. However, when the entire training dataset is considered, the accuracy is 100 percent. This emphasises the issue of low datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmqIov0DKhj"
      },
      "outputs": [],
      "source": [
        "model_election_neg = LogisticRegression(max_iter=10000, multi_class='multinomial', C=100)\n",
        "\n",
        "#Fit the model\n",
        "model_election_neg.fit(X_elec_comb_tr_tf,y_elec_comb_tr)\n",
        "\n",
        "#Predict\n",
        "y_preds_neg_tr = model_election_neg.predict(X_elec_comb_tr_tf)\n",
        "\n",
        "#Accuracy\n",
        "accs_neg_tr = metrics.accuracy_score(y_elec_comb_tr, y_preds_neg_tr)\n",
        "\n",
        "print(\"The accuracy score for the Train negative tweets: {}\".format(accs_neg_tr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPSY4gIZM6Gp"
      },
      "outputs": [],
      "source": [
        "def visualize(y_true, y_preds):\n",
        "  f1_cal = metrics.f1_score(y_true, y_preds, average=None)\n",
        "  cm = confusion_matrix(y_true, y_preds)\n",
        "\n",
        "  fig,ax= plt.subplots(figsize=(10,5))\n",
        "  sns.heatmap(cm, annot=True, ax = ax,fmt='g', cmap=\"YlGnBu\");\n",
        "\n",
        "  # labels, title and ticks\n",
        "  ax.set_xlabel('Predicted labels');\n",
        "  ax.set_ylabel('True labels');\n",
        "  ax.set_title('Confusion Matrix for Canadian Elections Dataset');\n",
        "  \n",
        "\n",
        "  #Calculating F1 score for each class using sklearn function\n",
        "  print(\"\\nThe F1 score for each class is: {}\".format(f1_cal))\n",
        "  plt.show()\n",
        "  \n",
        "  can_ori = np.sum(cm, axis=1)\n",
        "  can_pre = np.sum(cm, axis=0)\n",
        "\n",
        "\n",
        "  ind = np.arange(7)\n",
        "  fig,ax= plt.subplots(figsize=(10,5))\n",
        "  rects1 = ax.bar(ind, can_ori, width=0.1, color='blue', label='True Class')\n",
        "  rects2 = ax.bar(ind+0.1, can_pre, width=0.1, color='green', label='Predicted Class')\n",
        "  labels= ('Others', 'Exploitation', \n",
        "              'early election', 'Economy', 'gun control',\n",
        "              'Climate Problem', 'Healthcare')\n",
        "  plt.xticks(ind, labels,rotation='vertical', fontsize=\"10\")\n",
        "  plt.legend();\n",
        "  plt.title(\"Class Distribution\");\n",
        "  plt.xlabel('Classes');\n",
        "  plt.ylabel('Counts');\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q1G93MeNL-x"
      },
      "outputs": [],
      "source": [
        "visualize(y_elec_comb_tr, y_preds_neg_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwKwdDzRFIC5"
      },
      "outputs": [],
      "source": [
        "#Test and prediction Accuracies on Canadian Election dataset\n",
        "y_preds_neg_test = model_election_neg.predict(X_elec_comb_test_tf)  ## Using this for further prediction\n",
        "\n",
        "acc_neg_test = metrics.accuracy_score(y_elec_comb_test, y_preds_neg_test)\n",
        "report_neg_test = classification_report(y_elec_comb_test,y_preds_neg_test)\n",
        "\n",
        "#Print Accuracies for comparison\n",
        "print(\"The accuracy score for the Test negative tweets is: {0:.3}\".format(acc_neg_test))\n",
        "print(\"\\nThe classification report for the Test negative tweets is :\\n {}\".format(report_neg_test))\n",
        "visualize(y_elec_comb_test, y_preds_neg_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlAnWW8TYyc"
      },
      "source": [
        "## **4.a Reason for Poor Performance**\n",
        "\n",
        "One of the reasons for the model's poor performance is the dataset's severe class imbalance. Over predictions can be seen in the classes with the highest representation: Class 0 and Class 5. Because of the small number of datapoints, the model is unable to generalise well to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxtrF_oDTuoD"
      },
      "source": [
        "## **4.b Suggestion for improvement of model performance**\n",
        "\n",
        "One method to improve the performance of the model is to generate synthetic datapoints which can improve model generalization. \n",
        "\n",
        "Some methods to develop synthetic datapoints are:\n",
        "1. OverSampling \n",
        "2. UnderSampling\n",
        "3. SMOTE\n",
        "\n",
        "```\n",
        "#Examples:\n",
        "from imblearn.over_sampling import SMOTE\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLn4eFXcycn-"
      },
      "source": [
        "##Conclusion\n",
        "\n",
        "All the learning objectives set at the start of the assignment is applied and met.\n",
        "\n",
        "-The application of NLP concepts is applied in this assignment.\n",
        "\n",
        "-Various methods for pasring and cleaning the data is implemented.\n",
        "\n",
        "-Several text categorization models were investigated, and hyperparameter tweaking was applied to machine learning algorithms to the task of text classificationtraining and testing machine learning algorithms (logistic regression, k-NN, decision trees, random forest, XGBoost, etc.).\n",
        "\n",
        "-Overall, this project featured a variety of concepts to better understand Sentiment Analysis implementation, develop debates around the observed results, and even provide techniques to improve the implementation.\n",
        "\n",
        "- NLP either through tweeter or any social media platform gives a great idea to analyse the sentiment of large demography across any given region, Thus helps to predict possible correct consequences of any upcoming event like elections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "verma_finalproject_1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}